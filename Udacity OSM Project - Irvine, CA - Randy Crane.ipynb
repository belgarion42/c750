{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Project Outline:\n",
    "\n",
    "1) Introduction - DONE\n",
    "2) Import map data - DONE\n",
    "3) Create sample of of map data - DONE\n",
    "4) Basic overview of data - DONE\n",
    "5) List/explain 2 or 3 problems found in data (eg. abbreviated street names, incorrect postal codes, ???) - DONE\n",
    "6) Correct problem 1 - DONE\n",
    "7) Correct problem 2 - N/A\n",
    "8) Correct problem 3 (if applicable) - N/A\n",
    "9) Export data as csv file\n",
    "10) Import csv file into sql\n",
    "11) Compute overview of staistics of file:\n",
    "    a) size of the file\n",
    "    b) number of unique users\n",
    "    c) number of nodes and ways\n",
    "    d) number of chosen type of nodes, like cafes, shops etc.\n",
    "    e) Top 10 contributing users\n",
    "    f) Most popular cuisines?\n",
    "12) Conclusion\n",
    "    a) Ideas for improving the data set more\n",
    "    b) discussion about the benefits as well as some anticipated problems in implementing the improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "\n",
    "In this project I will be exploring OpenStreetMap data for Irvine, California. I chose this map data because Irvine is my current city of residence. I am already pretty familiar with the area, which will potentially help me recognize some of the bad data in the set. At the same time, I'm curious about some aspects of my city and I will use this data set to glean some new insights about it.\n",
    "\n",
    "The data consists of XML elements called \"nodes\" (points of interest) and \"ways\" (linear features and area boundaries). Each element can have one or more tags associated with it. These tags provide more details and information about the element.  Additional information about the data structure can be found here: https://wiki.openstreetmap.org/wiki/OSM_XML\n",
    "\n",
    "I will first explore the raw XML data using Python and audit tags that are appear to have errors, and tags that I'm interested in analyzing in more detail. Due to the scope of this project, I will not be cleaning every error I find, but will focus on two or three to meet the requirements of the project. After identifying and cleaning the problem areas, I will export the data to CSV files, then import it into a SQLite database. Finally, I will query the database to gain greater insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Map Area</h1>\n",
    "\n",
    "On OpenStreetMap.org, the area I selected to include as much of Irvine I could without too much data from other cities is bounded within the following coordinates:\n",
    "\n",
    "Longitude: -117.8792, -117.7281\n",
    "Latitude: 33.7301, 33.6135\n",
    "\n",
    "The map can be found here: https://www.openstreetmap.org/search?query=Irvine%2C%20CA#map=12/33.7299/-117.8232. The underlying data was downloaded using the Overpass API, at this link: https://overpass-api.de/api/map?bbox=-117.8792,33.6135,-117.7281,33.7301.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\Documents\\School - WGU\\Term 4\\C750\\Project\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Exploration and Cleaning</h1>\n",
    "\n",
    "Using the 'OSM_sampling.py' script provided by Udacity, I will create a sample file of every 10th element from the original file map data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing modules\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import osm file and create sample dataset\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "OSM_FILE = \"map.osm\"\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "            \n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write(b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write(b'<osm>\\n  ')\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "    output.write(b'</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will parse the sample dataset to count the unique element types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(filename, events=('start', )):\n",
    "        if elem.tag not in tags:\n",
    "            tags[elem.tag] = 1\n",
    "        else:\n",
    "            tags[elem.tag] += 1\n",
    "    return tags\n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('sample.osm')\n",
    "    pprint.pprint(tags)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the count_tags function, I got the number of unique tags shown. The ones that I expect to be of the most interest to me are:\n",
    "\n",
    "<ul>\n",
    "    <li>Nodes</li>\n",
    "    <li>Tags</li>\n",
    "    <li>Ways</li>\n",
    "    <li>Members</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, I will check the \"k\" value for each \"<tag>\" to see if there are any potential problems. There are three regular expressions: \n",
    "    \n",
    "<ol>\n",
    "    <li>\"lower\", for tags that contain only lowercase letters and are valid,</li>\n",
    "    <li>\"lower_colon\", for otherwise valid tags with a colon in their names,</li>\n",
    "    <li>\"problemchars\", for tags with problematic characters, and</li>\n",
    "    <li>\"other\", for other tags that do not fall into the other three categories.</li>\n",
    "</ol>\n",
    "    \n",
    "We need a count of each of these four tag categories in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if lower.search(element.attrib['k']):\n",
    "            keys['lower'] +=1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys['lower_colon']+=1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys['problemchars']+=1\n",
    "        else:\n",
    "            keys['other']+=1\n",
    "        \n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    keys = process_map('sample.osm')\n",
    "    pprint.pprint(keys)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we continue to explore the data, let's find out how many unique users have contributed to the data in this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        try:\n",
    "            users.add(element.attrib['uid'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return users\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    users = process_map('sample.osm')\n",
    "    pprint.pprint(len(users))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to check the validity and consistency of the street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSMFILE = \"sample.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\",\n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Layout\", \"Main\", \"Broadway\", \"Plaza\", \"Park\"]\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def print_sorted_dic(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print(\"%s: %d\" % (k, v))\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit():\n",
    "#    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(OSMFILE, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    pprint.pprint(dict(street_types))\n",
    "#    osm_file.close()\n",
    "#    return street_types\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The street name data is remarkably clean, though not perfect. Let's clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"St\": \"Street\",\n",
    "           \"ST\": \"Street\",\n",
    "           \"St.\": \"Street\",\n",
    "           \"St,\": \"Street\",\n",
    "           \"Street.\": \"Street\",\n",
    "           \"street\": \"Street\",\n",
    "           \"Sq\": \"Square\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Rd\": \"Road\",\n",
    "           \"Ave\": \"Avenue\",\n",
    "           \"DR.\": \"Drive\"\n",
    "           }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osmfile, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    for key, value in mapping.items():\n",
    "        if re.search(key, name):\n",
    "            name = re.sub(street_type_re, value, name)\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print (name, \"=>\", better_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, though it does introduce a new problem: names that start with \"St\" (e.g. \"Stanza\") are changed to \"Street,\" which is obviously not what was intended. If this project's purpose was to fully clean the data 100%, I would add code to exclude those those exceptions. However, as a proof of concept for the methodology this was successful, so we will move on. \n",
    "\n",
    "Let's look at the zip codes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a dictionary of our postal codes\n",
    "def audit_postal_code(postal_code_types, postal_code):  \n",
    "    if not postal_code.isupper() or ' ' not in postal_code:\n",
    "        postal_code_types['Postal Codes'].add(postal_code)\n",
    "    else:\n",
    "        postal_code_types['other'].add(postal_code)\n",
    "    return postal_code_types\n",
    "\n",
    "def is_postal_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit(filename):\n",
    "    f = (filename)\n",
    "    postal_code_types = defaultdict(set)\n",
    "    \n",
    "    for event, element in ET.iterparse(f, events=(\"start\",)):\n",
    "        if element.tag ==\"way\":\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if is_postal_code(tag):\n",
    "                    audit_postal_code(postal_code_types, tag.attrib['v'])\n",
    "    print(dict(postal_code_types))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit(OSMFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSMFILE = \"sample.osm\"\n",
    "postal_code_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"92602\", \"92603\", \"92604\", \"92606\", \"92612\", \"92614\", \"92616\", \"92617\", \n",
    "            \"92618\", \"92619\", \"92620\", \"92623\", \"92650\", \"92697\"]\n",
    "\n",
    "\n",
    "def audit_postal_code(postal_code_types, postal_codes):\n",
    "    m = postal_code_type_re.search(postal_codes)\n",
    "    if m:\n",
    "        postal_code = m.group()\n",
    "        if postal_code not in expected:\n",
    "            postal_code_types[postal_code].add(postal_code)\n",
    "\n",
    "def is_postal_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "def audit():\n",
    "    postal_codes = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(OSMFILE, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postal_code(tag):\n",
    "                    audit_postal_code(postal_codes, tag.attrib['v'])\n",
    "    \n",
    "    print(\"Postal codes in data set that are not in Irvine\")\n",
    "    pprint.pprint(dict(postal_codes))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what unique keys there are, and how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_keys(filename):\n",
    "    distinct_keys=[]\n",
    "    count=1\n",
    "\n",
    "    EL=get_element(filename, tags=('node', 'way', 'relation'))\n",
    "    for element in EL:\n",
    "        if element.tag=='node' or element.tag=='way':\n",
    "            for tag in element.iter('tag'):\n",
    "                if tag.attrib['k'] not in distinct_keys:\n",
    "                    distinct_keys.append(tag.attrib['k'])\n",
    "                    count+=1\n",
    "    distinct_keys.sort()\n",
    "    print(\"Total number of unique keys (tag attrib['k'])is {}:\".format(count))\n",
    "    \n",
    "#    return distinct_keys\n",
    "      \n",
    "    pprint.pprint(distinct_keys)\n",
    "    \n",
    "                \n",
    "unique_keys(OSMFILE)  # Using Sample file as input to audit the addr:street key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few \"Fix Me\" tags. I won't be cleaning those today, but they definitely indicate a problem with the data set that it would help users of OpenStreetMaps to resolve.\n",
    "\n",
    "Let's also take a look at phone number formats and state names before we do the analysis and cleaning of the full data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_phone(elem):\n",
    "    return (elem.attrib['k'] == \"phone\" or elem.attrib['k'] == \"contact:phone\")\n",
    "\n",
    "def audit_phone_number_formats(phone_number_formats, phone_number):\n",
    "\n",
    "    # Convert any digit to an 'X' sign (e.g. '(212) 333-3100' becomes '(XXX) XXX-XXXX')\n",
    "    phone_number_format = re.sub('\\d', 'X', phone_number)\n",
    "    phone_number_formats[phone_number_format] += 1\n",
    "\n",
    "def update_phone_number(phone_number):\n",
    "    # Keep the first phone number if more than one is present\n",
    "    if re.search(r';', phone_number):\n",
    "        phone_number = phone_number.split(';')[0] # Phone numbers are separated by ';'\n",
    "    elif re.search(r'/', phone_number):\n",
    "        phone_number = phone_number.split('/')[0] # Phone numbers are separated by '/'\n",
    "    \n",
    "    digits = re.sub('\\D', '', phone_number)\n",
    "    if len(digits) == 11: # 1XXXXXXXXXX\n",
    "        return '+' + digits[0] + '-' + digits[1:4] + '-' + digits[4:7] + '-' + digits[7:]\n",
    "    elif len(digits) == 10: # XXXXXXXXXX\n",
    "        return '+1' + '-' + digits[:3] + '-' + digits[3:6] + '-' + digits[6:]\n",
    "    elif len(digits) == 12: # 01XXXXXXXXXX\n",
    "        return '+' + digits[1] + '-' + digits[2:5] + '-' + digits[5:8] + '-' + digits[8:]\n",
    "    elif len(digits) == 13: # 001XXXXXXXXXX\n",
    "        return '+' + digits[2] + '-' + digits[3:6] + '-' + digits[6:9] + '-' + digits[9:]\n",
    "    else:\n",
    "        return phone_number\n",
    "    \n",
    "def audit(osmfile):\n",
    "    osm_file = OSMFILE\n",
    "    phone_number_formats = defaultdict(int)\n",
    "    for event, elem in ET.iterparse(osm_file, events=('start',)):\n",
    "\n",
    "        if elem.tag == 'node' or elem.tag == 'way':\n",
    "            for tag in elem.iter('tag'):\n",
    "                # Audit phone numbers\n",
    "                if is_phone(tag):\n",
    "                    audit_phone_number_formats(phone_number_formats, tag.attrib['v'])\n",
    "    pprint.pprint(dict(phone_number_formats))\n",
    "\n",
    "audit(OSM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_state(elem):\n",
    "    return (elem.attrib['k'] == \"addr:state\")\n",
    "\n",
    "state_types = defaultdict(int)\n",
    "\n",
    "def audit_state(state_types, state_name):\n",
    "    if state_name != 'CA':\n",
    "        state_types[state_name] += 1\n",
    "        print(state_name)\n",
    "\n",
    "for event, elem in ET.iterparse(OSMFILE, events =(\"start\",)):\n",
    "    if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "        for tag in elem.iter(\"tag\"):\n",
    "            if is_state(tag):\n",
    "                audit_state(state_types, tag.attrib['v'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step I'm going to take before reshaping the data and exporting it to CSV files is to clean up the street names for the whol data set, not just the same set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSMFILE = \"map.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\",\n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Layout\", \"Main\", \"Broadway\", \"Plaza\", \"Park\"]\n",
    "\n",
    "mapping = {\"St\": \"Street\",\n",
    "           \"ST\": \"Street\",\n",
    "           \"St.\": \"Street\",\n",
    "           \"St,\": \"Street\",\n",
    "           \"Street.\": \"Street\",\n",
    "           \"street\": \"Street\",\n",
    "           \"Sq\": \"Square\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Rd\": \"Road\",\n",
    "           \"Ave\": \"Avenue\",\n",
    "           \"DR.\": \"Drive\",\n",
    "           \"Blvd\": \"Boulevard\"\n",
    "           }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osmfile, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    for key, value in mapping.items():\n",
    "        if re.search(key, name):\n",
    "            name = re.sub(street_type_re, value, name)\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print (name, \"=>\", better_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Reshaping and Exporting to CSV Files</h1>\n",
    "\n",
    "OK, it's time to actually reshape/clean the data and get it exported to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema.py\n",
    "\n",
    "schema = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSM_PATH = \"sample.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "    if element.tag=='node':\n",
    "        for key in  element.attrib.keys():\n",
    "            if key in node_attr_fields:\n",
    "                node_attribs[key] = element.attrib[key]\n",
    "#         print node_attribs\n",
    "\n",
    "    if  element.findall('tag'):\n",
    "        for elem in element.iter('tag'):\n",
    "            tag={}\n",
    "            tag['id']=element.attrib['id']\n",
    "            tag['value']=elem.attrib['v']\n",
    "            if ':' in elem.attrib['k']:\n",
    "                type = re.split(':',elem.attrib['k'])[0]\n",
    "                tag['key']=elem.attrib['k'][len(type)+1:]\n",
    "                tag['type']=type\n",
    "            else:\n",
    "                tag['key']=elem.attrib['k']\n",
    "                tag['type']='regular'\n",
    "            if  LOWER_COLON.match(tag['key']) and PROBLEMCHARS.match(tag['key']):\n",
    "                print(tag['key'])\n",
    "            else:\n",
    "                tags.append(tag)\n",
    "#     print tags\n",
    "#     print '\\n'\n",
    "    \n",
    "    if element.tag=='way':\n",
    "        for key in element.attrib.keys():\n",
    "            if key in way_attr_fields:\n",
    "                way_attribs[key] = element.attrib[key]\n",
    "#         print way_attribs\n",
    "\n",
    "   \n",
    "    \n",
    "    if element.tag =='way':\n",
    "       \n",
    "        i=0  \n",
    "        for tag in element.iter('nd'):\n",
    "            temp={}\n",
    "            temp['id']=element.attrib['id']\n",
    "            temp['node_id']= tag.attrib['ref']\n",
    "            temp['position']= i\n",
    "            i+=1\n",
    "            way_nodes.append(temp)\n",
    "        \n",
    "        \n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "        \n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=schema):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, str) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)\n",
    "\n",
    "print(\"Reshaping and export complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('irvine_osm_database.sqlite')\n",
    "cursor = conn.cursor()\n",
    "print(\"Opened database successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    database = r\"D:\\Documents\\School - WGU\\Term 4\\C750\\Project\\irvine_osm_database.db\"\n",
    "\n",
    "    sql_create_nodes_table = \"\"\"CREATE TABLE IF NOT EXISTS nodes (\n",
    "        id INTEGER PRIMARY KEY NOT NULL,\n",
    "        lat REAL,\n",
    "        lon REAL,\n",
    "        user TEXT,\n",
    "        uid INTEGER,\n",
    "        version INTEGER,\n",
    "        changeset INTEGER,\n",
    "        timestamp TEXT\n",
    "    );\"\"\"\n",
    "\n",
    "    sql_create_nodes_tags_table = \"\"\"CREATE TABLE IF NOT EXISTS nodes_tags (\n",
    "        id INTEGER,\n",
    "        key TEXT,\n",
    "        value TEXT,\n",
    "        type TEXT,\n",
    "        FOREIGN KEY (id) REFERENCES nodes(id)\n",
    "    );\"\"\"\n",
    "\n",
    "\n",
    "    sql_create_ways_table = \"\"\"CREATE TABLE IF NOT EXISTS ways (\n",
    "        id INTEGER PRIMARY KEY NOT NULL,\n",
    "        user TEXT,\n",
    "        uid INTEGER,\n",
    "        version TEXT,\n",
    "        changeset INTEGER,\n",
    "        timestamp TEXT\n",
    "    );\"\"\"\n",
    "\n",
    "    sql_create_ways_tags_table = \"\"\"CREATE TABLE IF NOT EXISTS ways_tags (\n",
    "        id INTEGER NOT NULL,\n",
    "        key TEXT NOT NULL,\n",
    "        value TEXT NOT NULL,\n",
    "        type TEXT,\n",
    "        FOREIGN KEY (id) REFERENCES ways(id)\n",
    "    );\"\"\"\n",
    "\n",
    "    sql_create_ways_nodes_table = \"\"\"CREATE TABLE ways_nodes (\n",
    "        id INTEGER NOT NULL,\n",
    "        node_id INTEGER NOT NULL,\n",
    "        position INTEGER NOT NULL,\n",
    "        FOREIGN KEY (id) REFERENCES ways(id),\n",
    "        FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    "    );\"\"\"\n",
    "\n",
    "    \n",
    "   # create a database connection\n",
    "#    conn = create_connection(irvine_osm_database.db)\n",
    "\n",
    "    # create tables\n",
    "    if conn is not None:\n",
    "        # create nodes table\n",
    "        create_table(conn, sql_create_nodes_table)\n",
    "\n",
    "        # create nodes tags table\n",
    "        create_table(conn, sql_create_nodes_tags_table)\n",
    "        \n",
    "        # create ways table\n",
    "        create_table(conn, sql_create_ways_table)\n",
    "\n",
    "        # create ways tags table\n",
    "        create_table(conn, sql_create_ways_tags_table)\n",
    "\n",
    "        # create ways nodes table\n",
    "        create_table(conn, sql_create_ways_nodes_table)\n",
    "        \n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")\n",
    "        \n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "DATADIR = \"D:\\Documents\\School - WGU\\Term 4\\C750\\Project\"\n",
    "DATAFILE = \"nodes.csv\"\n",
    "\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    n = 0\n",
    "    with open(datafile, 'r') as sd:\n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r:\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    parse_csv(datafile)\n",
    "    d = parse_csv(datafile)\n",
    "    pprint.pprint(d, depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = \"nodes_tags.csv\"\n",
    "\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    n = 0\n",
    "    with open(datafile, 'r') as sd:\n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r:\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    parse_csv(datafile)\n",
    "    d = parse_csv(datafile)\n",
    "    pprint.pprint(d, depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = \"ways.csv\"\n",
    "\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    n = 0\n",
    "    with open(datafile, 'r') as sd:\n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r:\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    parse_csv(datafile)\n",
    "    d = parse_csv(datafile)\n",
    "    pprint.pprint(d, depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = \"ways_nodes.csv\"\n",
    "\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    n = 0\n",
    "    with open(datafile, 'r') as sd:\n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r:\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    parse_csv(datafile)\n",
    "    d = parse_csv(datafile)\n",
    "    pprint.pprint(d, depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = \"ways_tags.csv\"\n",
    "\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    n = 0\n",
    "    with open(datafile, 'r') as sd:\n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r:\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    parse_csv(datafile)\n",
    "    d = parse_csv(datafile)\n",
    "    pprint.pprint(d, depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"Select * FROM ways_nodes\")\n",
    "results = cursor.fetchall()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep these after any queries\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
